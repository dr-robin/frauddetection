{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-means\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "X = np.array([[1, 2], [1, 4], [1, 0],\n",
    "               [10, 2], [10, 4], [10, 0]])\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    "kmeans.labels_\n",
    "kmeans.predict([[0, 0], [12, 3]])\n",
    "\n",
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mini batch Kmeans\n",
    "'''ses mini-batches to reduce the computation time,\n",
    "while still attempting to optimise the same objective function.\n",
    "Mini-batches are subsets of the input data, randomly sampled\n",
    "in each training iteration. These mini-batches drastically\n",
    "reduce the amount of computation required to converge to a local solution\n",
    "'''\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import numpy as np\n",
    "X = np.array([[1, 2], [1, 4], [1, 0],\n",
    "               [4, 2], [4, 0], [4, 4],\n",
    "               [4, 5], [0, 1], [2, 2],\n",
    "               [3, 2], [5, 5], [1, -1]])\n",
    "# manually fit on batches\n",
    "kmeans = MiniBatchKMeans(n_clusters=2,\n",
    "                          random_state=0,\n",
    "                          batch_size=6)\n",
    "kmeans = kmeans.partial_fit(X[0:6,:])\n",
    "kmeans = kmeans.partial_fit(X[6:12,:])\n",
    "kmeans.cluster_centers_\n",
    "kmeans.predict([[0, 0], [4, 4]])\n",
    "\n",
    "# fit on the whole data\n",
    "kmeans = MiniBatchKMeans(n_clusters=2,\n",
    "                          random_state=0,\n",
    "                          batch_size=6,\n",
    "                          max_iter=10).fit(X)\n",
    "kmeans.cluster_centers_\n",
    "kmeans.predict([[0, 0], [4, 4]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Affinity propagation\n",
    "'''AffinityPropagation creates clusters by sending messages\n",
    "between pairs of samples until convergence. A dataset is then\n",
    "described using a small number of exemplars, which are identified\n",
    "as those most representative of other samples. '''\n",
    "\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "import numpy as np\n",
    "X = np.array([[1, 2], [1, 4], [1, 0],[4, 2], [4, 4], [4, 0]])\n",
    "clustering = AffinityPropagation(random_state=5).fit(X)\n",
    "clustering\n",
    "clustering.labels_\n",
    "clustering.predict([[0, 0], [4, 4]])\n",
    "clustering.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Meanshift clustering\n",
    "from sklearn.cluster import MeanShift\n",
    "import numpy as np\n",
    "X = np.array([[1, 1], [2, 1], [1, 0],[4, 7], [3, 5], [3, 6]])\n",
    "clustering = MeanShift(bandwidth=2).fit(X)\n",
    "clustering.labels_\n",
    "clustering.predict([[0, 0], [5, 5]])\n",
    "clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spectral clustering\n",
    "from sklearn.cluster import SpectralClustering\n",
    "import numpy as np\n",
    "X = np.array([[1, 1], [2, 1], [1, 0],\n",
    "               [4, 7], [3, 5], [3, 6]])\n",
    "clustering = SpectralClustering(n_clusters=2, assign_labels=\"discretize\",\n",
    "         random_state=0).fit(X)\n",
    "clustering.labels_\n",
    "\n",
    "clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ward hierachical clustering\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "clustering = AgglomerativeClustering(n_clusters=2, *,\n",
    "                affinity='euclidean', memory=None,\n",
    "                connectivity=None, compute_full_tree='auto',\n",
    "                linkage='ward', distance_threshold=None)\n",
    "\n",
    "#can select different linkage\n",
    "\n",
    "#linkage='ward'\n",
    "#linkage='complete\n",
    "#linkage='average\n",
    "#linkage='single'\n",
    "\n",
    "'''The linkage criterion determines which distance\n",
    "to use between sets of observation. The algorithm\n",
    "will merge the pairs of cluster that minimize this criterion.\n",
    "ward minimizes the variance of the clusters being merged.\n",
    "average uses the average of the distances of each observation\n",
    "of the two sets. omplete or maximum linkage uses\n",
    "the maximum distances between all observations of the two sets.\n",
    "single uses the minimum of the distances between all\n",
    "observations of the two sets.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature agglometrative clustering\n",
    "#good for feature selection\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets, cluster\n",
    "digits = datasets.load_digits()\n",
    "images = digits.images\n",
    "X = np.reshape(images, (len(images), -1))\n",
    "agglo = cluster.FeatureAgglomeration(n_clusters=32)\n",
    "agglo.fit(X)\n",
    "\n",
    "X_reduced = agglo.transform(X)\n",
    "X_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agglomerative clustering\n",
    "'''Agglomerative cluster has a “rich get richer” behavior that\n",
    "leads to uneven cluster sizes. In this regard, single linkage\n",
    "is the worst strategy, and Ward gives the most regular sizes.\n",
    "However, the affinity (or distance used in clustering) cannot\n",
    "be varied with Ward, thus for non Euclidean metrics, average\n",
    "linkage is a good alternative. Single linkage, while not robust \n",
    "to noisy data, can be computed very efficiently and can therefore\n",
    "be useful to provide hierarchical clustering of larger datasets.\n",
    "Single linkage can also perform well on non-globular data.\n",
    "'''\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np\n",
    "X = np.array([[1, 2], [1, 4], [1, 0],[4, 2], [4, 4], [4, 0]])\n",
    "clustering = AgglomerativeClustering().fit(X)\n",
    "clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DBSCAN\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "X = np.array([[1, 2], [2, 2], [2, 3],[8, 7], [8, 8], [25, 80]])\n",
    "clustering = DBSCAN(eps=3, min_samples=2).fit(X)\n",
    "clustering.labels_\n",
    "clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Optics\n",
    "'''OPTICS (Ordering Points To Identify the Clustering Structure),\n",
    "closely related to DBSCAN, finds core sample of high density and\n",
    "expands clusters from them [1]. Unlike DBSCAN, keeps cluste\n",
    "hierarchy for a variable neighborhood radius. Better suited\n",
    "for usage on large datasets than the current sklearn implementation of DBSCAN.'''\n",
    "from sklearn.cluster import OPTICS\n",
    "import numpy as np\n",
    "X = np.array([[1, 2], [2, 5], [3, 6],[8, 7], [8, 8], [7, 3]])\n",
    "clustering = OPTICS(min_samples=2).fit(X)\n",
    "clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Birch\n",
    "'''\n",
    "Birch or MiniBatchKMeans?\n",
    "Birch does not scale very well to high dimensional data. As a rule of thumb if n_features is greater than twenty, it is generally better to use MiniBatchKMeans.\n",
    "If the number of instances of data needs to be reduced, or if one wants a large number of subclusters either as a preprocessing step or otherwise, Birch is more useful than MiniBatchKMeans.\n",
    "'''\n",
    "\n",
    "from sklearn.cluster import Birch\n",
    "X = [[0, 1], [0.3, 1], [-0.3, 1], [0, -1], [0.3, -1], [-0.3, -1]]\n",
    "brc = Birch(n_clusters=None)\n",
    "brc.fit(X)\n",
    "brc.predict(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
